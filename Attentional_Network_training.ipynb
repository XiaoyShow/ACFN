{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attentional Correlation Filter Network for Adaptive Visual Tracking\n",
    "\n",
    "Jongwon Choi, 2017\n",
    "https://sites.google.com/site/jwchoivision/\n",
    "\n",
    "Python Code to Train Attentional Network\n",
    "\n",
    "Running environment:\n",
    "Linux Ubuntu 14.04.5 LTS\n",
    "ipython 5.1.0\n",
    "tensorflow 0.10.0rc0\n",
    "Cuda Release 8.0, V8.0.26\n",
    "\n",
    "When you use this code for your research, please refer the below reference.\n",
    "You can't use this code for any commercial purpose without author's agreement.\n",
    "If you have any question or comment, please contact to jwchoi.pil@gmail.com.\n",
    "\n",
    "Reference\n",
    "\n",
    "[1] Jongwon Choi, Hyung Jin Chang, Sangdoo Yun, Tobias Fischer, Yiannis Demiris, and Jin Young Choi, \"Attentional Correlation Filter Network for Adaptive Visual Tracking\", CVPR2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from os import listdir\n",
    "import scipy.io as sio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Path to the training data (ground truth)\n",
    "folder_name = \"./VOT_training_data/\"\n",
    "\n",
    "# Number of neurons in each fully-connected layer\n",
    "num_hidden = 1024\n",
    "\n",
    "# Number of lstm modules\n",
    "num_lstm = 256\n",
    "\n",
    "# Number of full-searching frames for initialization\n",
    "hierarchy_size = 5\n",
    "\n",
    "# Recurrent size\n",
    "truncated_size_rnn = 10    # >= hierarchy_size\n",
    "\n",
    "# Training batch size\n",
    "size_batch = 100\n",
    "\n",
    "# Number of tracking modules\n",
    "num_module = 260"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Real Samples\n",
    "training_scores = []\n",
    "training_distances = []\n",
    "\n",
    "folder_list = [f for f in listdir(folder_name)]\n",
    "idx = 0\n",
    "for i in range(len(folder_list)):\n",
    "    file_list = [f for f in listdir(folder_name+folder_list[i])]\n",
    "    #print file_list\n",
    "    \n",
    "    for j in range(len(file_list)):\n",
    "        file_name = folder_name+folder_list[i]+'/'+file_list[j]\n",
    "        loaded_data = sio.loadmat(file_name)\n",
    "        if(len(training_scores)==0):\n",
    "            training_scores = loaded_data['confidence_stack']\n",
    "            training_distances = loaded_data['distance_stack']\n",
    "            idx_frames = range(truncated_size_rnn+hierarchy_size, np.size(training_scores,0),1)\n",
    "        else:\n",
    "            training_scores = np.concatenate((training_scores, loaded_data['confidence_stack']))        \n",
    "            training_distances = np.concatenate((training_distances, loaded_data['distance_stack']))\n",
    "            idx_frames = np.concatenate((idx_frames, range(idx_frames[-1]+truncated_size_rnn+hierarchy_size, np.size(training_scores,0),1)))\n",
    "\n",
    "num_samples = np.size(training_scores, 0)\n",
    "\n",
    "#normalization (0~1)\n",
    "shapes = np.shape(training_scores)\n",
    "max_scores = np.max(training_scores, axis=1, keepdims=True)\n",
    "min_scores = np.min(training_scores, axis=1, keepdims=True)\n",
    "\n",
    "training_scores = (training_scores - min_scores) / (max_scores - min_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Variable initialization functions\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    #initial = tf.zeros(shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    #initial = tf.zeros(shape)\n",
    "    return tf.Variable(initial)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialization\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Place holders\n",
    "x = tf.placeholder(\"float\", shape=[None, truncated_size_rnn, num_module])\n",
    "y_ = tf.placeholder(\"float\", shape=[None, num_module])\n",
    "\n",
    "# Variblaes for training\n",
    "y_gt_temp = tf.slice(x, [0,1,0], [tf.size(x)/truncated_size_rnn/num_module, truncated_size_rnn-1,num_module])\n",
    "y_gt = tf.reshape(tf.transpose(tf.concat(1, [y_gt_temp, tf.reshape(y_,[-1,1,num_module])]),[1,0,2]), [-1,num_module])\n",
    "y_gt_list = tf.split(0, truncated_size_rnn, y_gt)\n",
    "score_x = tf.reshape(tf.transpose(x, [1, 0, 2]), shape=[-1, num_module])\n",
    "x_list = tf.split(1, truncated_size_rnn, x)\n",
    "prev_score_x = tf.reshape(x_list[-1], shape=[-1, num_module])\n",
    "\n",
    "\n",
    "## Prediction Sub-network ##\n",
    "#LSTM\n",
    "h_fc2_split = tf.split(0, truncated_size_rnn, score_x)\n",
    "lstm_cell = rnn_cell.BasicLSTMCell(num_lstm, forget_bias=1.0, state_is_tuple=True)\n",
    "\n",
    "h_rnn, state = rnn.rnn(lstm_cell, h_fc2_split, dtype=tf.float32)\n",
    "\n",
    "# fc1 + relu\n",
    "W_fc3 = weight_variable([num_lstm, num_hidden])\n",
    "b_fc3 = bias_variable([num_hidden])\n",
    "\n",
    "h_fc3 = tf.nn.relu(tf.matmul(h_rnn[-1], W_fc3) + b_fc3)\n",
    "\n",
    "# fc2 + relu\n",
    "W_fc3_2 = weight_variable([num_hidden, num_hidden])\n",
    "b_fc3_2 = bias_variable([num_hidden])\n",
    "\n",
    "h_fc3_2 = tf.nn.relu(tf.matmul(h_fc3, W_fc3_2) + b_fc3_2)\n",
    "\n",
    "# fc3 + relu\n",
    "W_fc3_3 = weight_variable([num_hidden, num_hidden])\n",
    "b_fc3_3 = bias_variable([num_hidden])\n",
    "\n",
    "h_fc3_3 = tf.nn.relu(tf.matmul(h_fc3_2, W_fc3_3) + b_fc3_3)\n",
    "\n",
    "# fc4\n",
    "W_fc4 = weight_variable([num_hidden, num_module])\n",
    "b_fc4 = bias_variable([num_module])\n",
    "\n",
    "h_fc4 = tf.matmul(h_fc3_3, W_fc4) + b_fc4\n",
    "\n",
    "# the predicted scores from the prediction network\n",
    "pred_score = h_fc4\n",
    "\n",
    "\n",
    "\n",
    "## Selection Sub-network ##\n",
    "# fc1 + relu\n",
    "W_fc5 = weight_variable([num_module, num_hidden])\n",
    "b_fc5 = bias_variable([num_hidden])\n",
    "\n",
    "h_fc5 = tf.nn.relu(tf.matmul(h_fc4, W_fc5) + b_fc5)\n",
    "\n",
    "# fc2 + relu\n",
    "W_fc5_2 = weight_variable([num_hidden, num_hidden])\n",
    "b_fc5_2 = bias_variable([num_hidden])\n",
    "\n",
    "h_fc5_2 = tf.nn.relu(tf.matmul(h_fc5, W_fc5_2) + b_fc5_2)\n",
    "\n",
    "# fc3\n",
    "W_fc6 = weight_variable([num_hidden, num_module])\n",
    "b_fc6 = bias_variable([num_module])\n",
    "\n",
    "h_fc6 = tf.matmul(h_fc5_2, W_fc6) + b_fc6\n",
    "\n",
    "\n",
    "## Network for top-k selection\n",
    "# binary result selecting the modules with high predicted score\n",
    "num_select = tf.placeholder(tf.int32)\n",
    "top_sel_val, top_sel_idx = tf.nn.top_k(tf.square(pred_score-y_), k=num_select)\n",
    "sliced_top_sel_val = tf.slice(top_sel_val, [0, num_select-1], [tf.size(top_sel_val)/num_select, 1])\n",
    "repeated_top_sel_val = tf.tile(tf.reshape(sliced_top_sel_val, [-1, 1]), [1, num_module])\n",
    "top_sel_thresholded = tf.to_float(tf.greater_equal(tf.square(pred_score-y_), repeated_top_sel_val))\n",
    "\n",
    "\n",
    "# pointer for predicted score\n",
    "curr_pred_score = pred_score\n",
    "\n",
    "\n",
    "## Network for top-k selection\n",
    "# binary result selecting the modules with high predicted score\n",
    "top_N = tf.placeholder(tf.int32) # number of the modules with high predicted score (parameter)\n",
    "top_N_val, top_N_idx = tf.nn.top_k(pred_score, k=top_N)\n",
    "sliced_top_N_val = tf.slice(top_N_val, [0, top_N-1], [tf.size(top_N_val)/top_N, 1])\n",
    "repeated_top_N_val = tf.tile(tf.reshape(sliced_top_N_val, [-1, 1]), [1, num_module])\n",
    "top_N_thresholded = tf.to_float(tf.greater_equal(pred_score, repeated_top_N_val))\n",
    "\n",
    "\n",
    "# Temporary (Not used)\n",
    "lambda_exp = tf.placeholder(\"float\")\n",
    "\n",
    "\n",
    "## Integration of the two results (error prediction + high predicted score)\n",
    "h_sel = tf.maximum(top_N_thresholded, tf.tanh(10*h_fc6))\n",
    "final_top_sel_val, final_top_sel_idx = tf.nn.top_k(h_sel, k=num_select)\n",
    "final_sliced_top_sel_val = tf.slice(final_top_sel_val, [0, num_select-1], [tf.size(final_top_sel_val)/num_select, 1])\n",
    "final_repeated_top_sel_val = tf.tile(tf.reshape(final_sliced_top_sel_val, [-1, 1]), [1, num_module])\n",
    "# pointer for binary selection of attentional modules\n",
    "final_top_sel_thresholded = tf.to_float(tf.greater_equal(h_sel, final_repeated_top_sel_val))\n",
    "h_sel_list = tf.split(0, truncated_size_rnn, h_sel)\n",
    "\n",
    "# final score estimation (for training)\n",
    "final_pred = tf.add(tf.mul(y_, h_sel), tf.mul(pred_score, 1-h_sel))\n",
    "\n",
    "# pointer for final score (for training)\n",
    "curr_final_pred = final_pred\n",
    "\n",
    "\n",
    "## Saver\n",
    "saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## For training\n",
    "# parameters for loss estimation\n",
    "learning_rate_part = tf.placeholder(\"float\")\n",
    "learning_rate = tf.placeholder(\"float\")\n",
    "lambda_sparse = tf.placeholder(\"float\")\n",
    "epsilon = tf.placeholder(\"float\")\n",
    "\n",
    "# Loss estimation\n",
    "error = tf.reduce_mean(tf.square(final_pred - y_)) + lambda_sparse*tf.reduce_mean(tf.log(epsilon+tf.abs(h_fc6)))\n",
    "error_part = tf.reduce_sum(tf.square(pred_score-y_))\n",
    "\n",
    "## Training pointer for selection sub-network\n",
    "var_list2 = [W_fc5, W_fc5_2,W_fc6, b_fc5, b_fc5_2, b_fc6]\n",
    "opt2 = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads = tf.gradients(error, var_list2)\n",
    "train_op = opt2.apply_gradients(zip(grads, var_list2))\n",
    "train_step = train_op\n",
    "\n",
    "## Training pointer for prediction sub-network\n",
    "train_step_part = tf.train.AdamOptimizer(learning_rate_part).minimize(error_part)\n",
    "\n",
    "# temporary. (for display)\n",
    "final_error_part = tf.reduce_mean(tf.square(curr_pred_score-y_))\n",
    "final_error = tf.reduce_mean(tf.square(curr_final_pred-y_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Function definition to configure one training batch\n",
    "def load_batch(training_scores, idx_frame_, curr_pos, n_samples, time_steps):\n",
    "    \n",
    "    # Flag to detect the end of data\n",
    "    changed = 0\n",
    "    \n",
    "    # reshape the input data\n",
    "    x_temp = np.reshape(training_scores[(idx_frame_[curr_pos]-time_steps):(idx_frame_[curr_pos]),:], [1,time_steps,-1])\n",
    "    y_temp = np.reshape(training_scores[idx_frame_[curr_pos],:], [1,-1])\n",
    "    \n",
    "    # configure the input data with ground truth\n",
    "    for i in xrange(curr_pos+1, curr_pos+n_samples, 1):                \n",
    "        x_temp = np.concatenate((x_temp, np.reshape(training_scores[(idx_frame_[i]-time_steps):(idx_frame_[i]),:], [1,time_steps,-1])), axis=0)\n",
    "        y_temp = np.concatenate((y_temp, np.reshape(training_scores[idx_frame_[i],:], [1,-1])), axis=0)\n",
    "\n",
    "    # Detect the end of data\n",
    "    curr_pos = curr_pos + n_samples\n",
    "    if(curr_pos + n_samples >= len(idx_frame_)):\n",
    "        curr_pos = 0\n",
    "        #Shuffle the data for next epoch\n",
    "        np.random.shuffle(idx_frame_)\n",
    "        changed = 1\n",
    "                    \n",
    "    # Return\n",
    "    return {'x':x_temp, 'y':y_temp, 'idx_frames':idx_frame_, 'curr_pos':curr_pos, 'changed':changed}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Session 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial : training error 15.6293 [loss : 406362]\n",
      "Naive model saved in file: ./model_VOT_v36_naive.ckpt\n"
     ]
    }
   ],
   "source": [
    "## Training session 1 to train the prediction sub-network.\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config=tf.ConfigProto(log_device_placement=True)\n",
    "config.gpu_options.allow_growth=True\n",
    "\n",
    "# Session open\n",
    "with tf.Session(config = config) as sess:\n",
    "    \n",
    "    # initialization\n",
    "    training_error = 0\n",
    "    training_loss = 0\n",
    "    curr_pos = 0\n",
    "    curr_epoch = 0\n",
    "    curr_idx_frame = np.copy(idx_frames)\n",
    "    cnt = 0\n",
    "    \n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    \n",
    "    # Estimate the initial loss and error\n",
    "    for i in range(10000):\n",
    "        \n",
    "        full_batch = load_batch(training_scores, curr_idx_frame, curr_pos, 100, truncated_size_rnn)\n",
    "        curr_pos = full_batch['curr_pos']\n",
    "        full_feed_dicts = {x: full_batch['x'], y_: full_batch['y']}\n",
    "        training_errors = sess.run( [final_error_part, error_part], feed_dict=full_feed_dicts)\n",
    "        \n",
    "        training_error = training_error + training_errors[0]\n",
    "        training_loss = training_loss + training_errors[1]\n",
    "        cnt = cnt + 1\n",
    "        \n",
    "        if full_batch['changed']==1:\n",
    "            break\n",
    "        \n",
    "    print(\"initial : training error %g [loss : %g]\"%(training_error/cnt, training_loss/cnt))\n",
    "    \n",
    "    # training parameter setting\n",
    "    learning_rate_input = 1e-3\n",
    "    \n",
    "    # re-initialization\n",
    "    training_error = 0\n",
    "    training_loss = 0\n",
    "    curr_pos = 0\n",
    "    curr_epoch = 0\n",
    "    curr_idx_frame = np.copy(idx_frames)\n",
    "    cnt = 0\n",
    "\n",
    "    # Training session start!\n",
    "    for i in range(1000): #for i in range(1000000):\n",
    "        \n",
    "        batch = load_batch(training_scores, curr_idx_frame, curr_pos, size_batch, truncated_size_rnn)\n",
    "        curr_idx_frame = batch['idx_frames']\n",
    "        curr_pos = batch['curr_pos']\n",
    "        \n",
    "        # For each epoch, save the trained network & print the loss and error.\n",
    "        if batch['changed'] == 1:            \n",
    "            save_path = saver.save(sess, \"./tmp/model_VOT_naive.ckpt\")\n",
    "            print(\"step %d (epoch %d), training error %g [loss : %g]\"%(i, curr_epoch, training_error / cnt, training_loss / cnt))\n",
    "            training_error = 0\n",
    "            training_loss = 0\n",
    "            cnt = 0\n",
    "            curr_epoch = curr_epoch + 1 \n",
    "        \n",
    "        # Training sequence\n",
    "        feed_dicts_train = {x: batch['x'], y_: batch['y'], learning_rate_part:learning_rate_input}\n",
    "        sess.run(train_step_part, feed_dict=feed_dicts_train)\n",
    "        \n",
    "        feed_dicts = {x: batch['x'], y_: batch['y'], learning_rate_part:learning_rate_input}\n",
    "        training_errors = sess.run( [final_error_part, error_part], feed_dict=feed_dicts)\n",
    "        training_error = training_error + training_errors[0]\n",
    "        training_loss = training_loss + training_errors[1]\n",
    "        cnt = cnt + 1\n",
    "    \n",
    "    # Save the final network\n",
    "    save_path = saver.save(sess, \"./model_VOT_naive.ckpt\")\n",
    "    print(\"Naive model saved in file: %s\" % save_path)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Session 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in file: ./model_VOT_v36_full.ckpt\n"
     ]
    }
   ],
   "source": [
    "## Training session 2 to train the selection sub-network.\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config=tf.ConfigProto(log_device_placement=True)\n",
    "config.gpu_options.allow_growth=True\n",
    "\n",
    "# Session open\n",
    "with tf.Session(config = config) as sess:\n",
    "    \n",
    "    # Load the pretrained network after the training session 1\n",
    "    saver.restore(sess, \"./model_VOT_naive.ckpt\")\n",
    "    \n",
    "    # Initialize the weights of the selection sub-network.\n",
    "    sess.run(tf.initialize_variables([W_fc5, W_fc5_2, W_fc6, b_fc5, b_fc5_2, b_fc6]))\n",
    "    \n",
    "    # parameter setting & variable initialization\n",
    "    curr_idx_frame = np.copy(idx_frames)\n",
    "    curr_pos = 0    \n",
    "    curr_epoch = 0\n",
    "    cnt = 0\n",
    "    training_error_part = 0\n",
    "    training_error_final = 0\n",
    "    training_error = 0\n",
    "    learning_rate_input = 1e-3\n",
    "         \n",
    "        \n",
    "    # Train selection sub-network\n",
    "    for i in range(1000): #for i in range(200000):\n",
    "        \n",
    "        batch = load_batch(training_scores, curr_idx_frame, curr_pos, size_batch, truncated_size_rnn)\n",
    "        curr_idx_frame = batch['idx_frames']        \n",
    "        curr_pos = batch['curr_pos']\n",
    "        \n",
    "        # For each epoch, save the trained network & print the loss and error.\n",
    "        if batch['changed'] == 1:            \n",
    "            save_path = saver.save(sess, \"./tmp/model_VOT_full.ckpt\")\n",
    "            print(\"step %d, training error %g (%g) [loss : %g]\"%(i, training_error_final/cnt, training_error_part/cnt, training_error/cnt))\n",
    "            training_error_part = 0\n",
    "            training_error_final = 0\n",
    "            training_error = 0\n",
    "            cnt = 0\n",
    "            \n",
    "            curr_epoch = curr_epoch + 1\n",
    "            \n",
    "        # Training sequence\n",
    "        feed_dicts_train = {x: batch['x'], y_: batch['y'], top_N:5, num_select:30, epsilon:1, lambda_sparse:0.1, learning_rate:learning_rate_input}\n",
    "        sess.run(train_step, feed_dict=feed_dicts_train)\n",
    "        \n",
    "        feed_dicts = {x: batch['x'], y_: batch['y'], top_N:5, num_select:30, epsilon:1, lambda_sparse:0.1, learning_rate:learning_rate_input}\n",
    "        training_errors = sess.run([final_error_part, final_error, error], feed_dict=feed_dicts)\n",
    "        \n",
    "        training_error_part = training_error_part + training_errors[0]\n",
    "        training_error_final = training_error_final + training_errors[1]\n",
    "        \n",
    "        training_error = training_error + training_errors[2]\n",
    "        cnt = cnt + 1\n",
    "        \n",
    "    # Save the final network\n",
    "    save_path = saver.save(sess, \"./model_VOT_full.ckpt\")\n",
    "    print(\"Model saved in file: %s\" % save_path)        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
